# Ollama 8K Context Solution - DEFINITIVE GUIDE

**Status**: ‚úÖ **WORKING** - Validated September 30, 2025  
**Performance Indicator**: Embedding requests take 300-700ms (vs ~50ms when truncated)

## TL;DR - Quick Fix Verification

```bash
# Test AppHost mode (recommended)
cd /workspaces/ActualGameSearch_V3
dotnet run --project src/ActualGameSearch.AppHost

# Test standalone mode (no Cosmos required)
INGESTION__REQUIRECOSMOS=false dotnet run --project src/ActualGameSearch.Worker -- health --embedding-health
```

**Success indicators:**
- Embedding requests take 300-700ms each (indicates full 8k context processing)
- Logs show: `DEBUG: actualContext=8192, embNumCtx=8192`
- Logs show: `Context validation passed. Proceeding with ingestion.`

## The Problem That Kept Coming Back

This "elegant genius solution" was repeatedly discovered, implemented, and then mysteriously appeared broken again. The root cause was **NOT** the Ollama model configuration - it was a **validation function bug**.

### What Was Happening

1. **Ollama Model**: ‚úÖ Correctly configured with 8k context via `nomic-embed-8k:latest`
2. **Runtime Parameters**: ‚úÖ Correctly set `num_ctx=8192` in both AppHost and standalone
3. **Validation Function**: ‚ùå **BUG** - Reading hardcoded `model_info.context_length: 2048` instead of actual runtime parameters

The validation function `GetActualOllamaContextAsync` was checking the base model's hardcoded limits instead of the actual runtime context being used, causing false failures.

## The Solution (September 2025)

### 1. Enhanced Validation Function

**File**: `src/ActualGameSearch.Worker/Program.cs`

The key fix was updating `GetActualOllamaContextAsync` to check runtime parameters first:

```csharp
private static async Task<int> GetActualOllamaContextAsync(HttpClient httpClient, string model)
{
    try
    {
        var response = await httpClient.PostAsync("/api/show", 
            new StringContent(JsonSerializer.Serialize(new { name = model }), 
            Encoding.UTF8, "application/json"));
        
        if (response.IsSuccessStatusCode)
        {
            var content = await response.Content.ReadAsStringAsync();
            var showResponse = JsonSerializer.Deserialize<JsonElement>(content);
            
            // CRITICAL: Check parameters first (runtime config)
            if (showResponse.TryGetProperty("parameters", out var parameters))
            {
                var paramString = parameters.GetString() ?? "";
                var match = Regex.Match(paramString, @"num_ctx\s+(\d+)", RegexOptions.IgnoreCase);
                if (match.Success && int.TryParse(match.Groups[1].Value, out var runtimeCtx))
                {
                    return runtimeCtx; // Return actual runtime context
                }
            }
            
            // Fallback: Check model_info (base model limits)
            if (showResponse.TryGetProperty("model_info", out var modelInfo) &&
                modelInfo.TryGetProperty("context_length", out var contextLength))
            {
                return contextLength.GetInt32();
            }
        }
    }
    catch (Exception ex)
    {
        Console.WriteLine($"Error getting actual context: {ex.Message}");
    }
    
    return 0;
}
```

### 2. Configuration Structure Alignment

**File**: `src/ActualGameSearch.Worker/appsettings.json`

Ensured configuration structure matches what the code expects:

```json
{
  "Ollama": {
    "Endpoint": "http://localhost:11434/",
    "Model": "nomic-embed-8k:latest"
  },
  "Embeddings": {
    "NumCtx": 8192
  }
}
```

## Architecture Overview

### Two Deployment Modes

| Mode | Ollama Version | Context Handling | Use Case |
|------|----------------|------------------|----------|
| **AppHost (Aspire)** | 0.3.12 (container) | ‚úÖ Perfect 8k support | Production, full orchestration |
| **Standalone** | 0.12.3 (host) | ‚úÖ Works with custom model | Development, testing |

### Custom Model Creation

The `nomic-embed-8k:latest` model is created via:

**File**: `infrastructure/Modelfile.nomic-embed-8k`
```
FROM nomic-embed-text:latest
PARAMETER num_ctx 8192
```

**Setup**: `infrastructure/setup-ollama-models.sh`
```bash
#!/bin/bash
ollama pull nomic-embed-text:v1.5
ollama create nomic-embed-8k:latest -f Modelfile.nomic-embed-8k
```

## Typical Run Instructions

### AppHost Mode (Recommended)

```bash
# Build AppHost
dotnet build src/ActualGameSearch.AppHost/ActualGameSearch.AppHost.csproj

# Run full stack with bronze ingestion
dotnet run --project src/ActualGameSearch.AppHost
```

### Standalone Worker Mode

```bash
# Health check only (no Cosmos required)
INGESTION__REQUIRECOSMOS=false dotnet run --project src/ActualGameSearch.Worker -- health --embedding-health

# Bronze ingestion (requires Cosmos)
dotnet run --project src/ActualGameSearch.Worker -- ingest bronze --sample=600 --reviews-cap-per-app=100
```

### VS Code Tasks

Available tasks in `.vscode/tasks.json`:
- **build AppHost**: `Ctrl+Shift+P` ‚Üí "Tasks: Run Task" ‚Üí "build AppHost"
- **run AppHost: worker bronze ingest**: Pre-configured bronze ingestion with optimal parameters

## Performance Validation

### Success Indicators

‚úÖ **Embedding Execution Times**: 300-700ms per request  
‚úÖ **Debug Logs**: `actualContext=8192, embNumCtx=8192`  
‚úÖ **Validation**: `Context validation passed. Proceeding with ingestion.`  
‚úÖ **Ollama Logs**: `llama_new_context_with_model: n_ctx = 8192`  

### Failure Indicators

‚ùå **Fast Times**: ~50ms per request (indicates 2k truncation)  
‚ùå **Validation Failure**: `CONTEXT VALIDATION FAILED: actualContext=2048, embNumCtx=8192`  
‚ùå **Wrong Model**: Using base `nomic-embed-text` instead of `nomic-embed-8k:latest`  

## Troubleshooting

### If Context Validation Fails

1. **Check model exists**: `ollama list | grep nomic-embed-8k`
2. **Recreate model if missing**: `cd infrastructure && ./setup-ollama-models.sh`
3. **Verify configuration**: Ensure `appsettings.json` has correct Ollama section
4. **Check validation function**: Ensure it reads `parameters` before `model_info`

### If Embedding Times Are Fast (~50ms)

This indicates context truncation. Check:
1. **Model in use**: Should be `nomic-embed-8k:latest`, not base model
2. **Ollama version**: AppHost uses 0.3.12 (better), host uses 0.12.3 (works with custom model)
3. **Runtime parameters**: Verify `num_ctx 8192` in model parameters

## Historical Context

This solution was originally discovered on **Day 5** but kept appearing broken due to the validation function bug. The pattern was:
1. ‚úÖ Implement working 8k model
2. ‚ùå Validation function reports failure (reading wrong metadata)  
3. üò§ Assume solution is broken, restart from scratch
4. üîÑ Repeat cycle

**Key Insight**: The solution was never broken - only the validation logic was faulty.

## Files Modified

- `src/ActualGameSearch.Worker/Program.cs` - Enhanced validation function
- `src/ActualGameSearch.Worker/appsettings.json` - Configuration alignment
- `infrastructure/Modelfile.nomic-embed-8k` - Custom model definition (existing)
- `infrastructure/setup-ollama-models.sh` - Model setup script (existing)

## Last Validated

**Date**: September 30, 2025  
**Context**: Both AppHost and standalone modes working perfectly  
**Performance**: 300-700ms embedding times confirming full 8k context processing  
**Validator**: Enhanced `GetActualOllamaContextAsync` function reading runtime parameters